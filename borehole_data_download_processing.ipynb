{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f87ea67",
   "metadata": {},
   "source": [
    "# Borehole Data Download and Processing Notebook\n",
    "This notebook downloads, processes, and saves borehole data from an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import os\n",
    "\n",
    "def ensure_directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def download_and_process_data(locations, start_time, end_time):\n",
    "    base_url = \"https://ntg.aquaticinformatics.net/Export/BulkExport\"\n",
    "    # Updated the DateRange to be custom with specific StartTime and EndTime\n",
    "    common_params = (f\"?DateRange=Custom&StartTime={start_time}&EndTime={end_time}&TimeZone=9.5&Calendar=CALENDARYEAR&\"\n",
    "                     \"Interval=PointsAsRecorded&Step=1&ExportFormat=csv&TimeAligned=True&\"\n",
    "                     \"RoundData=False&IncludeGradeCodes=False&IncludeApprovalLevels=False&\"\n",
    "                     \"IncludeQualifiers=False&IncludeInterpolationTypes=False\")\n",
    "\n",
    "    dataset_mapping = {\n",
    "        \"DepthBelowGround\": {\n",
    "            \"TS\": (\"DepthBelowGround.Publish\", 81),\n",
    "            \"FV\": (\"DepthBelowGround.Field Visits\", 81)\n",
    "        },\n",
    "        \"WaterElevation\": {\n",
    "            \"TS\": (\"Water Elevation (AHD).Publish\", 82),\n",
    "            \"FV\": (\"Water Elevation (AHD).Field Visits\", 82)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    base_output_dir = 'SRTM/bores'\n",
    "    ensure_directory_exists(base_output_dir)\n",
    "\n",
    "    success_list = []\n",
    "    failure_list = []\n",
    "    \n",
    "    for location in locations:\n",
    "        for dataset, types in dataset_mapping.items():\n",
    "            for dataset_type, (dataset_name, unit_id) in types.items():\n",
    "                full_url = (f\"{base_url}{common_params}&Datasets[0].DatasetName={dataset_name}%40{location}&\"\n",
    "                            f\"Datasets[0].Calculation=Instantaneous&Datasets[0].UnitId={unit_id}\")\n",
    "                print(f\"Constructed URL for {dataset} ({dataset_type}): {full_url}\")\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(full_url)\n",
    "                    response.raise_for_status()\n",
    "                    if response.ok:\n",
    "                        df = pd.read_csv(StringIO(response.text), skiprows=4, parse_dates=[0], index_col=[0])\n",
    "                        print(f\"Data for {location} - {dataset} ({dataset_type}):\")\n",
    "                        print(df.head())  # Print the first few rows of the data for debugging\n",
    "                        print(f\"Columns for {location} - {dataset} ({dataset_type}): {df.columns.tolist()}\")  # Print column names for debugging\n",
    "                    else:\n",
    "                        print(f\"Error fetching data from {full_url}: {response.status_code}\")\n",
    "                        failure_list.append((location, dataset, dataset_type, f\"HTTP Error {response.status_code}\"))\n",
    "                        continue\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error fetching data for {location} from {full_url}: {e}\")\n",
    "                    failure_list.append((location, dataset, dataset_type, str(e)))\n",
    "                    continue\n",
    "                except ValueError as ve:\n",
    "                    print(f\"Value Error for location {location}: {ve}\")\n",
    "                    failure_list.append((location, dataset, dataset_type, str(ve)))\n",
    "                    continue\n",
    "                \n",
    "                # Look for any column containing 'Value' and determine if it's in meters or inches\n",
    "                value_column = None\n",
    "                for col in df.columns:\n",
    "                    if 'Value' in col:\n",
    "                        value_column = col\n",
    "                        break\n",
    "\n",
    "                if value_column is None:\n",
    "                    print(f\"No 'Value (m)' or 'Value (in)' column found in the data for location {location}\")\n",
    "                    failure_list.append((location, dataset, dataset_type, \"No 'Value (m)' or 'Value (in)' column found\"))\n",
    "                    continue\n",
    "\n",
    "                # If the value is in inches, convert it to meters\n",
    "                if 'in' in value_column.lower():\n",
    "                    df['Value'] = df[value_column] * 0.0254  # Convert inches to meters\n",
    "                    df['Value'] = df['Value'].round(2)  # Round to 2 decimal places\n",
    "                else:\n",
    "                    df['Value'] = df[value_column]\n",
    "\n",
    "                try:\n",
    "                    # Keep only the 'Value' column in meters and rename it\n",
    "                    df = df[['Value']].rename(columns={'Value': 'Value (m)'})\n",
    "                    df = df.rename_axis(\"DateTime\").reset_index()\n",
    "                    df['Location'] = location\n",
    "\n",
    "                    # Create a directory based on dataset name and type\n",
    "                    output_dir = os.path.join(base_output_dir, f\"{dataset_name}_{dataset_type}\")\n",
    "                    ensure_directory_exists(output_dir)\n",
    "\n",
    "                    filename = f\"{output_dir}/{location}_{dataset}_{dataset_type}.csv\"\n",
    "                    df.to_csv(filename, index=False)\n",
    "                    print(f\"Saved {filename}\")\n",
    "                    success_list.append(filename)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing data for {location} for {dataset} ({dataset_type}): {e}\")\n",
    "                    failure_list.append((location, dataset, dataset_type, str(e)))\n",
    "\n",
    "    # Summary of download process\n",
    "    print(\"\n",
    "Download Summary:\")\n",
    "    print(f\"Total files downloaded successfully: {len(success_list)}\")\n",
    "    for success in success_list:\n",
    "        print(f\" - {success}\")\n",
    "\n",
    "    print(f\"\n",
    "Total files failed to download or process: {len(failure_list)}\")\n",
    "    for failure in failure_list:\n",
    "        location, dataset, dataset_type, error = failure\n",
    "        print(f\" - Location: {location}, Dataset: {dataset}, Type: {dataset_type}, Error: {error}\")\n",
    "\n",
    "    return success_list, failure_list\n",
    "\n",
    "# Read bore names from bore_ti_tree.csv\n",
    "def get_bore_names_from_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        return df['Site'].tolist()\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file 'bore_ti_tree.csv' was not found.\")\n",
    "        return []\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"The file 'bore_ti_tree.csv' is empty.\")\n",
    "        return []\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"There was an error parsing 'bore_ti_tree.csv'.\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "bore_names = get_bore_names_from_csv('Bores_new.xlsx')\n",
    "if bore_names:\n",
    "    # Specify your custom start and end times\n",
    "    start_time = \"1960-01-01 00:00:00\"\n",
    "    end_time = \"2024-12-31 00:00:00\"\n",
    "    \n",
    "    download_and_process_data(bore_names, start_time, end_time)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
